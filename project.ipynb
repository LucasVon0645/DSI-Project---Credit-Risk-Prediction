{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science I Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_table_with_credit_risk(index: str, df: pd.DataFrame):\n",
    "    cross_table_credit_hist = pd.crosstab(df[index], df[\"CreditRiskClass\"], margins=True)\n",
    "    cross_table_credit_hist[f\"P(Bad|{index})\"] = cross_table_credit_hist[\"Bad\"]/cross_table_credit_hist[\"All\"]\n",
    "    cross_table_credit_hist[f\"P(Good|{index})\"] = 1 - cross_table_credit_hist[f\"P(Bad|{index})\"]\n",
    "    return cross_table_credit_hist\n",
    "\n",
    "def detect_rare_categories(dataset_df: pd.DataFrame, threshold: int):\n",
    "    categorical_cols = dataset_df.select_dtypes(\"object\").columns\n",
    "    \n",
    "    for col_name in categorical_cols:\n",
    "        rel_freq_s = dataset_df[col_name].value_counts(normalize=True)*100\n",
    "        \n",
    "        below_threshold = rel_freq_s[rel_freq_s < threshold].index.to_list()\n",
    "        \n",
    "        if (len(below_threshold)): \n",
    "            print(f\"\\n Categories in Feature {col_name} below threshold freq: {below_threshold}\")\n",
    "            print(\"Relative Frequency: \")\n",
    "            print(rel_freq_s)\n",
    "\n",
    "def merge_categories(dataset_df: pd.DataFrame, feature: str, categories_to_merge: list, merged_category_name: str):\n",
    "    dataset_df[feature] = dataset_df[feature].apply(lambda value: merged_category_name if value in categories_to_merge else value)\n",
    "    return dataset_df\n",
    "\n",
    "def detect_rare_ordinals(dataset_df: pd.DataFrame, threshold: int):\n",
    "    numerical_cols = dataset_df.select_dtypes(\"int64\")\n",
    "    \n",
    "    col_unique_counts_s = numerical_cols.nunique()\n",
    "    \n",
    "    ordinal_cols = col_unique_counts_s[col_unique_counts_s < 8].index\n",
    "    \n",
    "    for col_name in ordinal_cols:\n",
    "        rel_freq_s = dataset_df[col_name].value_counts(normalize=True)*100\n",
    "        \n",
    "        below_threshold = rel_freq_s[rel_freq_s < threshold].index.to_list()\n",
    "        \n",
    "        if (len(below_threshold)): \n",
    "            print(f\"\\n Categories in Feature {col_name} below threshold freq: {below_threshold}\")\n",
    "            print(\"Relative Frequency: \")\n",
    "            print(rel_freq_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statlog_german_credit_data.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = statlog_german_credit_data.metadata\n",
    "\n",
    "print(\"Abstract: \", metadata[\"abstract\"])\n",
    "print(\"Area: \", metadata[\"area\"])\n",
    "print(\"Number of Instances: \", metadata[\"num_instances\"])\n",
    "print(\"Has Missing Values: \", metadata[\"has_missing_values\"])\n",
    "print(\"Number of Features: \", metadata[\"num_features\"])\n",
    "print(\"Feature Types: \", metadata[\"feature_types\"])\n",
    "print(\"Demographics: \", metadata[\"demographics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statlog_german_credit_data.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing (with some EDA)\n",
    "\n",
    "In this part, the categorical features will be transformed into easier understandable data, so that the EDA can be performed.\n",
    "Besides, categories in some features will be joined to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = statlog_german_credit_data.data.original\n",
    "\n",
    "COLUMN_RENAME_DICT = {\n",
    "    'Attribute1': 'CheckingAccountStatus',\n",
    "    'Attribute2': 'DurationInMonths',\n",
    "    'Attribute3': 'CreditHistory',\n",
    "    'Attribute4': 'Purpose',\n",
    "    'Attribute5': 'CreditAmount',\n",
    "    'Attribute6': 'SavingsAccountBonds',\n",
    "    'Attribute7': 'EmploymentSince',\n",
    "    'Attribute8': 'InstallmentRate',\n",
    "    'Attribute9': 'PersonalStatusSex',\n",
    "    'Attribute10': 'OtherDebtorsGuarantors',\n",
    "    'Attribute11': 'ResidenceSince',\n",
    "    'Attribute12': 'Property',\n",
    "    'Attribute13': 'Age',\n",
    "    'Attribute14': 'OtherInstallmentPlans',\n",
    "    'Attribute15': 'Housing',\n",
    "    'Attribute16': 'ExistingCreditsCount',\n",
    "    'Attribute17': 'Job',\n",
    "    'Attribute18': 'PeopleLiableMaintenance',\n",
    "    'Attribute19': 'Telephone',\n",
    "    'Attribute20': 'ForeignWorker',\n",
    "    'class': 'CreditRiskClass'\n",
    "}\n",
    "\n",
    "dataset_df = dataset_df.rename(columns = COLUMN_RENAME_DICT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the mappings for the categorical data are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated mapping dictionary for all categorical columns\n",
    "categorical_mappings = {\n",
    "    'CheckingAccountStatus': {\n",
    "        'A11': 'Less than 0 DM',\n",
    "        'A12': '0 to 200 DM',\n",
    "        'A13': '200 DM or more',\n",
    "        'A14': 'No checking account'\n",
    "    },\n",
    "    'CreditHistory': {\n",
    "        'A30': 'No credits taken', # individual has never taken out any credits\n",
    "        'A31': 'All credits at this bank paid back duly', # individual has taken out credits from this particular bank and has paid them all back on time\n",
    "        'A32': 'Existing credits paid back duly till now', # individual currently has existing credits and has been paying them back on time up to the present moment.\n",
    "        'A33': 'Delay in paying off in the past', # individual has experienced delays in making credit payments in the past\n",
    "        'A34': 'Critical account / other credits existing'\n",
    "    },\n",
    "    'Purpose': {\n",
    "        'A40': 'Car (new)',\n",
    "        'A41': 'Car (used)',\n",
    "        'A42': 'Furniture/equipment',\n",
    "        'A43': 'Radio/television',\n",
    "        'A44': 'Domestic appliances',\n",
    "        'A45': 'Repairs',\n",
    "        'A46': 'Education',\n",
    "        'A47': 'Vacation',\n",
    "        'A48': 'Retraining',\n",
    "        'A49': 'Business',\n",
    "        'A410': 'Others'\n",
    "    },\n",
    "    'SavingsAccountBonds': {\n",
    "        'A61': 'Less than 100 DM',\n",
    "        'A62': '100 to 500 DM',\n",
    "        'A63': '500 to 1000 DM',\n",
    "        'A64': '1000 DM or more',\n",
    "        'A65': 'Unknown / no savings account'\n",
    "    },\n",
    "    'EmploymentSince': {\n",
    "        'A71': 'Unemployed',\n",
    "        'A72': 'Less than 1 year',\n",
    "        'A73': '1 to 4 years',\n",
    "        'A74': '4 to 7 years',\n",
    "        'A75': '7 years or more'\n",
    "    },\n",
    "    'PersonalStatusSex': {\n",
    "        'A91': 'Male: divorced',\n",
    "        'A92': 'Female: divorced/married',\n",
    "        'A93': 'Male: single',\n",
    "        'A94': 'Male: married/widowed',\n",
    "        'A95': 'Female: single'\n",
    "    },\n",
    "    'OtherDebtorsGuarantors': {\n",
    "        'A101': 'None',\n",
    "        'A102': 'Co-applicant',\n",
    "        'A103': 'Guarantor'\n",
    "    },\n",
    "    'Property': {\n",
    "        'A121': 'Real estate',\n",
    "        'A122': 'Building society savings agreement / life insurance',\n",
    "        'A123': 'Car or other, not in attribute 6',\n",
    "        'A124': 'Unknown / no property'\n",
    "    },\n",
    "    'OtherInstallmentPlans': {\n",
    "        'A141': 'Bank',\n",
    "        'A142': 'Stores',\n",
    "        'A143': 'None'\n",
    "    },\n",
    "    'Housing': {\n",
    "        'A151': 'Rent',\n",
    "        'A152': 'Own',\n",
    "        'A153': 'For free'\n",
    "    },\n",
    "    'Job': {\n",
    "        'A171': 'Unemployed / unskilled - non-resident',\n",
    "        'A172': 'Unskilled - resident',\n",
    "        'A173': 'Skilled employee / official',\n",
    "        'A174': 'Management / self-employed / \\n highly qualified employee / officer'\n",
    "    },\n",
    "    'Telephone': {\n",
    "        'A191': 'None',\n",
    "        'A192': 'Yes, registered under the customer\\'s name'\n",
    "    },\n",
    "    'ForeignWorker': {\n",
    "        'A201': 'Yes',\n",
    "        'A202': 'No'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing codes with descriptive labels using the consolidated mappings\n",
    "for column, mapping in categorical_mappings.items():\n",
    "    dataset_df[column] = dataset_df[column].replace(mapping)\n",
    "\n",
    "# Separating Personal Status from Gender in original feature \"PersonalStatusSex\" \n",
    "dataset_df[\"Gender\"] = dataset_df[\"PersonalStatusSex\"].str.extract(\"([A-Za-z]+)\")\n",
    "dataset_df[\"PersonalStatus\"] = dataset_df[\"PersonalStatusSex\"].str.extract(\"(?<=:\\s)(.*)$\")\n",
    "dataset_df = dataset_df.drop(\"PersonalStatusSex\", axis=1)\n",
    "\n",
    "# Replace values of the target variable, as defined in the instructions of the dataset \n",
    "dataset_df[\"CreditRiskClass\"] = dataset_df[\"CreditRiskClass\"].replace(1,0) # Good payer, low risk\n",
    "dataset_df[\"CreditRiskClass\"] = dataset_df[\"CreditRiskClass\"].replace(2,1) # Bad payer. high risk\n",
    "\n",
    "credit_risk_class_map = [\"Good\", \"Bad\"]\n",
    "\n",
    "dataset_df[\"CreditRiskClass\"] = dataset_df[\"CreditRiskClass\"].apply(lambda risk_class: credit_risk_class_map[risk_class])\n",
    "\n",
    "print(\"New columns: \", dataset_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_rare_categories(dataset_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare categories are merged to reduce complexity and avoid overfitting\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"CreditHistory\",\n",
    "                 categories_to_merge=[\"All credits at this bank paid back duly\", \"No credits taken\"],\n",
    "                 merged_category_name=\"No credits taken or all credits paid back duly\")\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"Purpose\",\n",
    "                 categories_to_merge=[\"Retraining\", \"Domestic appliances\", \"Repairs\", \"Education\"],\n",
    "                 merged_category_name=\"Others\")\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"SavingsAccountBonds\",\n",
    "                              categories_to_merge=[\"500 to 1000 DM\", \"1000 DM or more\"],\n",
    "                              merged_category_name=\"500 DM or more\")\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"OtherDebtorsGuarantors\",\n",
    "                 categories_to_merge=[\"Guarantor\", \"Co-applicant\"],\n",
    "                 merged_category_name=\"Yes\")\n",
    "dataset_df[\"OtherDebtorsGuarantors\"] = dataset_df[\"OtherDebtorsGuarantors\"].replace(\"None\", \"No\")\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"OtherInstallmentPlans\",\n",
    "                 categories_to_merge=[\"Bank\", \"Stores\"],\n",
    "                 merged_category_name=\"Yes\")\n",
    "dataset_df[\"OtherInstallmentPlans\"] = dataset_df[\"OtherInstallmentPlans\"].replace(\"None\", \"No\")\n",
    "\n",
    "dataset_df = merge_categories(dataset_df, feature=\"Job\",\n",
    "                 categories_to_merge=[\"Unskilled - resident\", \"Unemployed / unskilled - non-resident\"],\n",
    "                 merged_category_name=\"Unemployed/Unskilled\")\n",
    "dataset_df[\"OtherInstallmentPlans\"] = dataset_df[\"OtherInstallmentPlans\"].replace(\"None\", \"No\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although only 37 samples out of 1000 have the ForeignWorker = 1, this feature could still be useful for the credit risk prediction. However, This is a sign that this feature could potentially lead to overfitting. Future steps will evaluate if this feature is necessary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_rare_ordinals(dataset_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_df.info())\n",
    "number_samples = len(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"CreditRiskClass\", data=dataset_df, hue=\"CreditRiskClass\")\n",
    "dataset_df[\"CreditRiskClass\"].value_counts()/number_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is highly unbalanced between the classes \"Good payers\" and \"Bad payers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset_df, hue=\"CreditRiskClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Univariate distributions\n",
    "Observing the diagonal, no features can clearly separate the two classes, what makes the classification task harder. The univariate distributions for both classes are general very similar. Apparently, the features \"CreditAmount\" and \"DurationInMonths\" are the ones how gives a higher difference in the distribution format.\n",
    "\n",
    "#### 2. DurationInMonths and CreditAmount\n",
    "In the features 'DurationInMonths' and 'CreditAmount', the distributions for each target class show a positive skewness (the mean is higher than the mode). In the target class \"bad payers\" however, the skewness seems to be higher (mean is further away from the mode). Besides, in the joint scatter plot DurationInMonths-CreditAmount, the concentration of \"bad payers\" is also higher for higher values of \n",
    "CreditAmount, specially for higher CreditAmount and lower DurationInMonths values.\n",
    "\n",
    "#### 3. Other Numerical Features\n",
    "It is possible to note that all numerical feature except for \"Age\", \"DurationInMonths\" and \"CreditAmount\" have only few discrete possible values (ordinal values). Therefore, a \"Spearman\" correlation measure is more suitable for correlation analysis, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = dataset_df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "numerical_features[\"CreditRisk\"] = dataset_df[\"CreditRiskClass\"].replace(to_replace={\"Good\": 0, \"Bad\": 1})\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(data=numerical_features.corr(method='spearman'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heatmap above, it is possible to see that the DurationInMonths and CreditAmount are the features with the highest correlation in absolute value, followed by \"InstallmentRate-CreditAmount\", \"ResidenceSince-Age\" and \"PeopleLiableMaintenance-Age\".\n",
    "\n",
    "What regards ranked correlation with the credit risk, with \"0\" denoting low risk (good payer) and \"1\" denoting high risk (bad payer), we can see that no feature has a high correlation in absolute value. The highest ones are \"DurationInMonths\", \"Age\", \"Credit Amount\", \"InstallmentRate\". The highest one (DurationInMonths) has only \"-0.27\". This suggests that techniques such as Logistic Regression would not perform good. As seen below, it would be harder to fit a logistic function in the graphics below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "sns.scatterplot(ax=axes[0], x=\"CreditAmount\", y=\"CreditRiskClass\", data=dataset_df, hue=\"CreditRiskClass\")\n",
    "axes[0].grid()\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=\"DurationInMonths\", y=\"CreditRiskClass\", data=dataset_df, hue=\"CreditRiskClass\")\n",
    "axes[1].grid()\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.scatterplot(ax=axes[2], x=\"Age\", y=\"CreditRiskClass\", data=dataset_df, hue=\"CreditRiskClass\")\n",
    "axes[2].grid()\n",
    "axes[2].set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(ax=axes[1], x=\"Gender\", hue=\"CreditRiskClass\", data=dataset_df, stat=\"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "\n",
    "sns.countplot(ax = axes[0], x=\"OtherInstallmentPlans\", hue=\"CreditRiskClass\", data=dataset_df)\n",
    "sns.countplot(ax = axes[1], x=\"OtherDebtorsGuarantors\", hue=\"CreditRiskClass\", data=dataset_df)\n",
    "sns.countplot(ax = axes[2], x=\"Housing\", hue=\"CreditRiskClass\", data=dataset_df)\n",
    "\n",
    "max_y = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1], axes[2].get_ylim()[1])\n",
    "\n",
    "# Set the same y-axis limit for both plots\n",
    "axes[0].set_ylim(0, max_y)\n",
    "axes[1].set_ylim(0, max_y)\n",
    "axes[2].set_ylim(0, max_y)\n",
    "\n",
    "#create_frequency_table_with_credit_risk(index=\"OtherInstallmentPlans\", df=dataset_df)\n",
    "create_frequency_table_with_credit_risk(index=\"OtherDebtorsGuarantors\", df=dataset_df)\n",
    "# create_frequency_table_with_credit_risk(index=\"Housing\", df=dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "sns.countplot(x=\"Job\", hue=\"CreditRiskClass\", data=dataset_df)\n",
    "plt.title(\"Job Qualification\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "create_frequency_table_with_credit_risk(index=\"Job\", df=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to note that the proportion of \"Good\" payers and \"Bad\" payers remains almost the same among different categories of jobs. It is also equal to the general percentage of good and bad payers (i.e. 70%/30%). Therefore, the feature regarding the job qualification/status seems to be not relevant for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_frequency_table_with_credit_risk(index=\"CreditHistory\", df=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"critical account history / other credits existing\", the rate between the target classes is quite different from the 70%/30% rate. However, for all other categories, the rate doesn't seem to change too much. Such result suggests that the relevance of this feature should be analyzed more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "order_checking = ['Less than 0 DM', '0 to 200 DM', '200 DM or more', 'No checking account']\n",
    "\n",
    "sns.countplot(x=\"CheckingAccountStatus\", hue=\"CreditRiskClass\", data=dataset_df, order=order_checking)\n",
    "\n",
    "create_frequency_table_with_credit_risk(index=\"CheckingAccountStatus\", df=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature \"CheckingAccountStatus\" has apparently a good capability to distinguish between the two classes. \n",
    "\n",
    "1. The proportion of \"bad payers\" in \"No checking account\" is very small. It is only 11%, what is quite different from the general 30/70% rate\n",
    "2. The rate of the two target classes is almost the same in the category \"Less than 0 DM\". This suggests that the there is significant increase of chance of a person being a bad payer if it has a negative checking account.\n",
    "3. The information of positive checking account offers a slight increase on the risk probability\n",
    "\n",
    "Such result suggests that this feature is relevant for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "order_savings = ['Less than 100 DM','100 to 500 DM','500 DM or more', 'Unknown / no savings account']\n",
    "sns.countplot(x=\"SavingsAccountBonds\", hue=\"CreditRiskClass\", data=dataset_df, order=order_savings)\n",
    "\n",
    "create_frequency_table_with_credit_risk(index=\"SavingsAccountBonds\", df=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data-Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection and Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
